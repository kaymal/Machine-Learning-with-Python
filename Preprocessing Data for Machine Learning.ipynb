{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data for Machine Learning\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "* Exploring and Preparing Data\n",
    "  - Checking Data Types\n",
    "  - Handling Missing Data\n",
    "      + Dropping Mising Data\n",
    "      + Imputation\n",
    "* Feature Scaling (Standardizing Data)\n",
    "  - Log normalization\n",
    "  - Scaling\n",
    "* Feature Engineering\n",
    "  - Numerical Features\n",
    "  - Categorical Features (Encoding)\n",
    "  - Text\n",
    "* Feature Selection\n",
    "* Data Splitting\n",
    "  - Class Distribution\n",
    "\n",
    "_Datasets: UFO sightings, Gapminder._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import re # Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>seconds</th>\n",
       "      <th>length_of_time</th>\n",
       "      <th>desc</th>\n",
       "      <th>recorded</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/3/2011 19:21</td>\n",
       "      <td>woodville</td>\n",
       "      <td>wi</td>\n",
       "      <td>us</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1209600.0</td>\n",
       "      <td>2 weeks</td>\n",
       "      <td>Red blinking objects similar to airplanes or s...</td>\n",
       "      <td>12/12/2011</td>\n",
       "      <td>44.9530556</td>\n",
       "      <td>-92.291111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/3/2004 19:05</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>oh</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30sec.</td>\n",
       "      <td>Many fighter jets flying towards UFO</td>\n",
       "      <td>10/27/2004</td>\n",
       "      <td>41.4994444</td>\n",
       "      <td>-81.695556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/25/2009 21:00</td>\n",
       "      <td>coon rapids</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green&amp;#44 red&amp;#44 and blue pulses of light tha...</td>\n",
       "      <td>12/12/2009</td>\n",
       "      <td>45.1200000</td>\n",
       "      <td>-93.287500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date         city state country     type    seconds  \\\n",
       "0  11/3/2011 19:21    woodville    wi      us  unknown  1209600.0   \n",
       "1  10/3/2004 19:05    cleveland    oh      us   circle       30.0   \n",
       "2  9/25/2009 21:00  coon rapids    mn      us    cigar        0.0   \n",
       "\n",
       "  length_of_time                                               desc  \\\n",
       "0        2 weeks  Red blinking objects similar to airplanes or s...   \n",
       "1         30sec.               Many fighter jets flying towards UFO   \n",
       "2            NaN  Green&#44 red&#44 and blue pulses of light tha...   \n",
       "\n",
       "     recorded         lat       long  \n",
       "0  12/12/2011  44.9530556 -92.291111  \n",
       "1  10/27/2004  41.4994444 -81.695556  \n",
       "2  12/12/2009  45.1200000 -93.287500  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the file\n",
    "df_ufo = pd.read_csv('datasets/ufo.csv')\n",
    "\n",
    "# Print the first 3 rows\n",
    "df_ufo.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date               object\n",
       "city               object\n",
       "state              object\n",
       "country            object\n",
       "type               object\n",
       "seconds           float64\n",
       "length_of_time     object\n",
       "desc               object\n",
       "recorded           object\n",
       "lat                object\n",
       "long              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the data types of the df\n",
    "print(df_ufo.dtypes)\n",
    "\n",
    "# Print the count of each data type\n",
    "print(df_ufo.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     11/3/2011 19:21\n",
      "1     10/3/2004 19:05\n",
      "2     9/25/2009 21:00\n",
      "3    11/21/2002 05:45\n",
      "4     8/19/2010 12:55\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the head of 'date' column\n",
    "print(df_ufo.date.head())\n",
    "\n",
    "# Change the date column to type datetime\n",
    "df_ufo[\"date\"] = pd.to_datetime(df_ufo[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    44.9530556\n",
      "1    41.4994444\n",
      "2    45.1200000\n",
      "3    36.0213889\n",
      "4     51.083333\n",
      "Name: lat, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the head of lat column.\n",
    "print(df_ufo.lat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'lat' column to type int\n",
    "df_ufo['lat'] = df_ufo['lat'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code gives an error, since there is one row in the lat column which is not a float (e.g. '2/3/2012'). There are 9 such wrong entries in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of rows in \"lat\" column which has '/' in its string\n",
    "df_ufo.lat.apply(lambda x: \"/\" in x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, some variables are actually categorical, and it is inefficient to work pandas `ojectt` type. We can use `.astype('category')` to change the data type of the variables to category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lambda function\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df.label = df[['label']].apply(categorize_label, axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row with wrong lat information\n",
    "df_ufo = df_ufo[df_ufo.lat.apply(lambda x: \"/\" not in x)]\n",
    "\n",
    "# Convert the 'lat' column to type int\n",
    "df_ufo['lat'] = df_ufo['lat'].astype(float)\n",
    "\n",
    "# Drop the row with wrong lat information\n",
    "#df_ufo.drop(df_ufo[df_ufo['lat']=='2/3/2012'].index, axis=0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to deal with wrong entries is encoding them as `NaN`. With this technique, we can deal with missing data easily and more efficiently. For instance, we can use `fillna()`, `dropna()`, and `Imputer()` with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert entries in \"lat\" column with '/' to NaN\n",
    "# df_ufo.lat[df_ufo.lat.apply(lambda x: \"/\" in x)] = np.nan\n",
    "\n",
    "# Print the number of NaNs\n",
    "# print(df_ufo.lat.isnull().sum())\n",
    "\n",
    "# Drop missing values and print shape of new DataFrame\n",
    "# df_ufo = df_ufo.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4926 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      "date              4926 non-null datetime64[ns]\n",
      "city              4926 non-null object\n",
      "state             4512 non-null object\n",
      "country           4255 non-null object\n",
      "type              4776 non-null object\n",
      "seconds           4926 non-null float64\n",
      "length_of_time    4785 non-null object\n",
      "desc              4926 non-null object\n",
      "recorded          4926 non-null object\n",
      "lat               4926 non-null float64\n",
      "long              4926 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(3), object(7)\n",
      "memory usage: 461.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the info for for the df_ufo\n",
    "df_ufo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2833    False\n",
       "443     False\n",
       "3562    False\n",
       "3736    False\n",
       "986     False\n",
       "Name: country, dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a sample of the country column and check whether it has missing value\n",
    "df_ufo.country.isnull().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                0\n",
       "city                0\n",
       "state             414\n",
       "country           671\n",
       "type              150\n",
       "seconds             0\n",
       "length_of_time    141\n",
       "desc                0\n",
       "recorded            0\n",
       "lat                 0\n",
       "long                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing values in all columns\n",
    "df_ufo.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of missing values in the 'city' column\n",
    "print(df_ufo.city.isnull().sum())\n",
    "\n",
    "# print the shape of the df\n",
    "print(df_ufo.shape)\n",
    "\n",
    "# Subset the dataset with missing values in the 'city' column and print the shape of the new df\n",
    "print(df_ufo[df_ufo.city.isnull()].shape)\n",
    "\n",
    "# Subset the dataset with non-missing values in the 'city' column and print the shape of the new df\n",
    "print(df_ufo[df_ufo.city.notnull()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "df_ufo = df_ufo[df_ufo.length_of_time.notnull() & \n",
    "          df_ufo.state.notnull() & \n",
    "          df_ufo.type.notnull() &\n",
    "          df_ufo.country.notnull()]\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "df_ufo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values in all columns\n",
    "df_ufo.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `dropna()` function to remove data, with parameters `axis=0` for rows and `thresh=` for the desired threshold. For instance, when threshold is 8, we'll drop the columns/features which have less than 8 non-missing values (or at least 4 missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with at least 4 missing values\n",
    "df_ufo.dropna(axis=0, thresh=8).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `fit()` method learns model parameters (e.g. mean and standard deviation for normalization) from a training set, and `transform()` method applies this transformation model to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Imputer module\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Setup the Imputation transformer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "\n",
    "# Fit\n",
    "imp.fit(X)\n",
    "\n",
    "# Transform\n",
    "X = imp.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gapminder Dataset\n",
    "\n",
    "EDA for the Gapminder dataset is done in another notebook. We'll just import the dataset, print some info and continue with the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the gapminder file into df_gm\n",
    "df_gm = pd.read_csv(\"datasets/gapminder.csv\")\n",
    "\n",
    "df_gm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the Gapminder dataset. all the variable types are float except for \"Region\". It is a categorical variable and will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gm.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "![Big and Small](https://eazybi.com/static/img/blog_page/posts/2015_12_14/small_vs_big.jpg \"Big and Small\")\n",
    "\n",
    "We may need to standardize the range of independent variables (or features) in order to increase predicton accuracy, especially when the range of values of different features varies significantly. Standardization is required for certain Machine Learning algorithms (such as K-nearest neighbors and SVM) to work properly. \n",
    "(Note: Wine dataset is used in this section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z = {x - \\mu \\over \\sigma} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ufo.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gm.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(wine.Proline.var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine.Proline)\n",
    "\n",
    "# Check the variance of the Proline column again\n",
    "print(wine.Proline.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data - Standardizing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scale\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Scale the features: X_scaled\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Print the mean and standard deviation of the unscaled features\n",
    "print(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \n",
    "print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n",
    "\n",
    "# Print the mean and standard deviation of the scaled features\n",
    "print(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \n",
    "print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the columns to average\n",
    "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']\n",
    "\n",
    "# Use apply to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(df_ufo.date.head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "df_ufo[\"month\"] = df_ufo[\"date\"].apply(lambda d: d.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "df_ufo[\"year\"] = df_ufo[\"date\"].apply(lambda d: d.year)\n",
    "\n",
    "# Print the head of all three columns\n",
    "print(df_ufo[['date', 'month', 'year']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "When a variable/feature has two categories (or more than two categories and the categorical feature is ordinal) we can use Scikit-Learn's `LabelEncoder()` to encode it into 1s and 0s. When it has more than two categories and the feature is not ordinal, then we can perform one-hot encoding with pandas `get_dummies()` or Scikit-learn `OneHotEncoder()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking.Accessible)\n",
    "\n",
    "# Compare the two columns\n",
    "hiking[['Accessible_enc', 'Accessible']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the Region column of the df_gm\n",
    "category_enc = pd.get_dummies(df_gm.Region)\n",
    "\n",
    "#category_enc = pd.get_dummies(df_gm.Region, prefix_sep='_')\n",
    "\n",
    "# Print the head of the encoded columns\n",
    "category_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Multiple Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "df.label = df[['label']].apply(categorize_label, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of the length_of_time column\n",
    "df_ufo.length_of_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "def return_minutes(time_string):\n",
    "\n",
    "    # Use \\d+ to grab digits\n",
    "    pattern = re.compile(r\"(\\d+).min\")\n",
    "\n",
    "    # Use match on the pattern and column\n",
    "    num = re.match(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(1)) # return the first phranthesized subgroup\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "df_ufo[\"minutes\"] = df_ufo[\"length_of_time\"].apply(lambda row: return_minutes(row))\n",
    "\n",
    "# Print the head of both of the columns\n",
    "df_ufo[['length_of_time', 'minutes']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y sets using train_test_split, without stratify\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y sets using train_test_split, without stratify\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the distribution of a column is uneven, we may use **stratifying** in order to train the model on a sample which is representative of the dataset.\n",
    "\n",
    "We want to predict which country the UFO sighting took place in. Therefore, we'll investigate the class distribution for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution (or balance) for country column.\n",
    "df_ufo[\"country\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
